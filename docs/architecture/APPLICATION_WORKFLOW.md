# EcoTrace - Complete Application Workflow Guide

**Date**: October 29, 2025
**Purpose**: Understanding how the entire application works

---

## ðŸŽ¯ System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     EcoTrace Application                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend   â”‚â—„â”€â”€â”€â”¤   Backend    â”‚â—„â”€â”€â”€â”¤  Databases   â”‚
â”‚  React App   â”‚    â”‚  FastAPI     â”‚    â”‚  4 Systems   â”‚
â”‚  Port: 5173  â”‚    â”‚  Port: 8000  â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                    â”‚
       â”‚                    â”‚                    â”œâ”€ Elasticsearch
       â”‚                    â”‚                    â”œâ”€ Neo4j
       â”‚                    â”‚                    â”œâ”€ MongoDB
       â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â””â”€ Redis
       â”‚
       â””â”€â”€â”€ User Interface (8 pages)
```

---

## ðŸ“Š Data Flow - How Everything Works

### **Step 1: Data Collection (Scrapy Crawlers)** ðŸ•·ï¸

**Location**: `/backend/scrapy_crawlers/ecotrace_crawler/`

**4 Specialized Spiders**:

1. **Corporate Spider** (`corporate_spider.py`):
   - Crawls company sustainability reports
   - Extracts: Company name, claims, goals, reports
   - Example: sustainability.google.com

2. **Regulatory Spider** (`regulatory_spider.py`):
   - Crawls EPA, SEC, UN Climate sites
   - Extracts: Compliance data, regulations, standards

3. **Scientific Spider** (`scientific_spider.py`):
   - Crawls research papers, journals
   - Extracts: Studies, data, validations

4. **News Spider** (`news_spider.py`):
   - Crawls news sites, press releases
   - Extracts: Articles, claims, controversies

**How to Run**:
```bash
cd backend/scrapy_crawlers/ecotrace_crawler
scrapy crawl corporate_spider -o output.json
```

---

### **Step 2: Data Processing Pipeline** âš™ï¸

**Location**: `backend/scrapy_crawlers/ecotrace_crawler/pipelines.py`

**6 Processing Steps**:

```python
Item â†’ Validation â†’ NLP Extraction â†’ Elasticsearch â†’ Neo4j â†’ MongoDB
```

#### **Pipeline 1: DataValidationPipeline** âœ…
```python
# Validates and cleans data
- Removes None values
- Sets default scores (0.5)
- Converts types (string â†’ float)
- Ensures required fields exist
```

#### **Pipeline 2: NLPExtractionPipeline** ðŸ§ 
```python
# Uses spaCy for text analysis
- Extracts entities (companies, dates, numbers)
- Calculates confidence scores
- Identifies key phrases
- Sentiment analysis
```

#### **Pipeline 3: ElasticsearchPipeline** ðŸ”
```python
# Indexes for full-text search
- Creates searchable indices
- Enables fast queries
- Stores: companies, claims, news, sources
```

#### **Pipeline 4: Neo4jPipeline** ðŸ•¸ï¸
```python
# Creates knowledge graph
- Maps relationships (Company â†’ Claims â†’ Sources)
- Creates nodes and edges
- Enables graph queries
```

#### **Pipeline 5: MongoDBPipeline** ðŸ“¦
```python
# Stores complete documents
- Full claim text
- All metadata
- Historical data
```

#### **Pipeline 6: RedisPipeline** âš¡
```python
# Caches frequently accessed data
- Fast retrieval
- Session storage
- Reduces database load
```

---

### **Step 3: Backend API (FastAPI)** ðŸš€

**Location**: `backend/api/main.py`

**API Architecture**:
```python
FastAPI App
â”œâ”€â”€ Health Check: /api/health
â”œâ”€â”€ Companies: /api/companies/
â”œâ”€â”€ Claims: /api/claims/
â”œâ”€â”€ Analytics: /api/analytics/overview
â”œâ”€â”€ Search: /api/search/
â””â”€â”€ Company Details: /api/companies/{id}/
```

**How It Works**:

1. **Request Received**:
   ```
   User clicks "Companies" â†’ Frontend sends GET /api/companies/
   ```

2. **Database Query**:
   ```python
   # From Elasticsearch
   results = es_client.search(index="companies", size=50)
   ```

3. **Data Processing**:
   ```python
   # Format and validate
   companies = [format_company(hit) for hit in results]
   ```

4. **Response Sent**:
   ```python
   return JSONResponse(companies)
   ```

**Key Endpoints Explained**:

#### **/api/health** - System Check
```python
Response:
{
  "status": "healthy",
  "services": {
    "elasticsearch": true,
    "neo4j": true,
    "mongodb": true
  }
}
```

#### **/api/companies/** - List All Companies
```python
Query: Elasticsearch "companies" index
Returns: Array of company objects with scores
```

#### **/api/companies/{id}** - Company Details
```python
Query: Elasticsearch by ID
Returns: Single company with full data
```

#### **/api/companies/{id}/claims** - Company Claims
```python
Query: Elasticsearch filtered by company_id
Returns: All claims for that company
```

#### **/api/claims/** - List All Claims
```python
Query: Elasticsearch "claims" index
Returns: Array of sustainability claims
```

#### **/api/analytics/overview** - Dashboard Stats
```python
Queries: Multiple databases
Aggregates: Counts, averages, trends
Returns: Statistics object
```

#### **/api/search/** - Full-Text Search
```python
Query: Elasticsearch with query string
Searches: Across all indices
Returns: Ranked results by relevance
```

---

### **Step 4: Frontend Application (React)** ðŸ’»

**Location**: `frontend/src/`

**8 Pages Architecture**:

```
App.jsx (Router)
â”œâ”€â”€ Landing.jsx          â†’ / (Home page)
â”œâ”€â”€ Dashboard.jsx        â†’ /dashboard (Analytics)
â”œâ”€â”€ Companies.jsx        â†’ /companies (List)
â”œâ”€â”€ CompanyDetail.jsx    â†’ /companies/:id (Details)
â”œâ”€â”€ Claims.jsx           â†’ /claims (All claims)
â”œâ”€â”€ Analytics.jsx        â†’ /analytics (Advanced)
â”œâ”€â”€ KnowledgeGraph.jsx   â†’ /knowledge-graph (Network)
â””â”€â”€ Search.jsx           â†’ /search (Search)
```

**How Frontend Works**:

#### **Data Fetching with React Query**:
```javascript
// In any component
const { data, isLoading, error } = useQuery({
  queryKey: ['companies'],
  queryFn: () => axios.get('/api/companies/'),
  staleTime: 5 * 60 * 1000  // 5 min cache
})
```

#### **Page Load Flow**:
```
1. User visits /companies
   â†“
2. React Router loads Companies.jsx
   â†“
3. useQuery triggers API call
   â†“
4. Loading state shows skeletons
   â†“
5. API responds with data
   â†“
6. React Query caches response
   â†“
7. Component renders with data
   â†“
8. User sees companies with logos
```

---

## ðŸ”„ Complete User Journey Example

### **Scenario: User Searches for Tesla Sustainability**

**Step-by-Step Flow**:

1. **User Opens Application**:
   ```
   Browser â†’ http://localhost:5173
   Landing Page loads with hero section
   ```

2. **User Navigates to Search**:
   ```
   Click "Search" in sidebar
   â†’ React Router: /search
   â†’ Search.jsx component loads
   ```

3. **User Types Query**:
   ```
   Input: "Tesla sustainability"
   State: query = "Tesla sustainability"
   ```

4. **User Clicks Search**:
   ```
   Form submit â†’ handleSearch()
   â†’ API Call: GET /api/search/?q=Tesla%20sustainability&limit=50
   â†’ Loading: true (skeleton cards show)
   ```

5. **Backend Processes Request**:
   ```python
   # FastAPI receives request
   query = "Tesla sustainability"

   # Elasticsearch query
   results = es_client.search(
       index="*",  # All indices
       query={
           "multi_match": {
               "query": query,
               "fields": ["name", "claim_text", "summary"]
           }
       }
   )

   # Return results
   return {"results": results}
   ```

6. **Frontend Receives Results**:
   ```javascript
   // Search.jsx
   setResults(data.results)  // e.g., 5 results
   setLoading(false)
   ```

7. **Results Display**:
   ```
   - 2 Company results (Tesla, Inc.)
   - 2 Claim results (Tesla's carbon goals)
   - 1 News result (Tesla news article)

   Each with:
   - Type badge (color-coded)
   - Title
   - Description
   - Metadata (scores, dates)
   - View Details button
   ```

8. **User Clicks "View Details" on Tesla**:
   ```
   Click â†’ Navigate to /companies/tesla-id
   â†’ CompanyDetail.jsx loads
   â†’ 3 API calls in parallel:
      1. GET /api/companies/tesla-id
      2. GET /api/companies/tesla-id/claims
      3. GET /api/companies/tesla-id/score
   â†’ Page shows:
      - Company info with logo
      - List of all claims
      - Credibility score chart
      - Related sources
   ```

9. **User Explores Knowledge Graph**:
   ```
   Click "Knowledge Graph" in sidebar
   â†’ KnowledgeGraph.jsx loads
   â†’ React Flow renders:
      - 30 nodes (companies, claims, sources, regulatory)
      - 25+ edges (relationships)
   â†’ User can:
      - Drag nodes
      - Click for details
      - Filter by type
      - Change layout
   ```

10. **User Views Dashboard**:
    ```
    Click "Dashboard"
    â†’ Dashboard.jsx loads
    â†’ GET /api/analytics/overview
    â†’ Displays:
       - 6 stat cards (companies, claims, scores)
       - 7 charts (trends, distributions, comparisons)
       - Alerts panel
       - Top performers leaderboard
       - Quick actions
    ```

---

## ðŸ” Deep Dive: How Credibility Scoring Works

### **Scoring Algorithm**:

```python
# In NLPExtractionPipeline

def calculate_credibility(claim):
    """
    Multi-factor scoring:
    1. Source credibility (0-1)
    2. Evidence strength (0-1)
    3. Specificity score (0-1)
    4. Temporal clarity (0-1)
    """

    # Factor 1: Source credibility
    source_score = 0.0
    if 'sec.gov' in source_url:
        source_score = 0.9  # Official filing
    elif 'company.com/sustainability' in source_url:
        source_score = 0.7  # Company report
    elif 'news.com' in source_url:
        source_score = 0.6  # News article

    # Factor 2: Evidence strength
    evidence_score = count_citations(text) / 10  # Max 1.0

    # Factor 3: Specificity
    specificity = has_numbers(text) and has_dates(text)
    specificity_score = 1.0 if specificity else 0.5

    # Factor 4: Temporal clarity
    temporal_score = 1.0 if has_target_year(text) else 0.6

    # Weighted average
    credibility = (
        source_score * 0.4 +
        evidence_score * 0.3 +
        specificity_score * 0.2 +
        temporal_score * 0.1
    )

    return credibility  # 0.0 to 1.0
```

### **Example Scoring**:

**High Credibility Claim (0.85)**:
```
"Tesla commits to 100% renewable energy in US operations by 2025,
as stated in their 2023 SEC 10-K filing, with $500M investment
in solar infrastructure."

Breakdown:
- Source: SEC filing (0.9)
- Evidence: Specific numbers, amount (0.8)
- Specificity: Has numbers and dates (1.0)
- Temporal: Clear target year (1.0)
â†’ Score: 0.85
```

**Low Credibility Claim (0.45)**:
```
"Company X wants to be more sustainable in the future."

Breakdown:
- Source: Generic website (0.5)
- Evidence: No citations (0.3)
- Specificity: Vague (0.5)
- Temporal: No target date (0.6)
â†’ Score: 0.45
```

---

## ðŸ—„ï¸ Database Relationships

### **How Data is Connected**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Company     â”‚â”€â”€â”€â”
â”‚  - ID        â”‚   â”‚
â”‚  - Name      â”‚   â”‚
â”‚  - Website   â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                   â”‚ makes
                   â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Claim      â”‚â”€â”€â”€â”
              â”‚  - ID        â”‚   â”‚
              â”‚  - Text      â”‚   â”‚
              â”‚  - Score     â”‚   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                 â”‚ verified_by
                                 â†“
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   Source     â”‚â”€â”€â”€â”
                            â”‚  - URL       â”‚   â”‚
                            â”‚  - Type      â”‚   â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                               â”‚ filed_with
                                               â†“
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚ Regulatory   â”‚
                                          â”‚  - Name      â”‚
                                          â”‚  - Type      â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example Neo4j Query**:
```cypher
// Find all claims by Tesla verified by SEC
MATCH (c:Company {name: "Tesla"})-[:MAKES]->(claim:Claim)
      -[:VERIFIED_BY]->(source:Source)-[:FILED_WITH]->(reg:Regulatory {name: "SEC"})
RETURN claim, source
```

---

## âš¡ Real-Time Features

### **How Updates Propagate**:

1. **New Data Crawled**:
   ```
   Scrapy crawls â†’ Pipeline processes â†’ Databases update
   ```

2. **User Refreshes Page**:
   ```
   React Query cache expires â†’ API called â†’ New data fetched
   ```

3. **Auto-Refresh** (if implemented):
   ```javascript
   useQuery({
     queryKey: ['companies'],
     queryFn: fetchCompanies,
     refetchInterval: 60000  // Every 1 minute
   })
   ```

---

## ðŸŽ¨ UI Component Tree

### **Dashboard Page Example**:

```
Dashboard.jsx
â”œâ”€â”€ Header
â”‚   â”œâ”€â”€ Title + Icon
â”‚   â”œâ”€â”€ Time Range Selector (7d, 30d, 90d, 1y)
â”‚   â””â”€â”€ Action Buttons (Export, Filters)
â”œâ”€â”€ Stats Grid
â”‚   â”œâ”€â”€ Stat Card (Total Companies)
â”‚   â”œâ”€â”€ Stat Card (Claims Analyzed)
â”‚   â”œâ”€â”€ Stat Card (Verified Claims)
â”‚   â”œâ”€â”€ Stat Card (Avg Credibility)
â”‚   â”œâ”€â”€ Stat Card (Active Alerts)
â”‚   â””â”€â”€ Stat Card (Data Sources)
â”œâ”€â”€ Alerts Panel
â”‚   â”œâ”€â”€ Warning Alert (Amber)
â”‚   â”œâ”€â”€ Success Alert (Green)
â”‚   â””â”€â”€ Info Alert (Blue)
â”œâ”€â”€ Charts Row 1
â”‚   â”œâ”€â”€ Area Chart (Verification Trends)
â”‚   â””â”€â”€ Pie Chart (Claim Types)
â”œâ”€â”€ Charts Row 2
â”‚   â”œâ”€â”€ Bar Chart (Industry Performance)
â”‚   â””â”€â”€ Radar Chart (System Metrics)
â”œâ”€â”€ Leaderboard
â”‚   â””â”€â”€ Top 5 Companies Table
â”œâ”€â”€ Distribution Bars
â”‚   â””â”€â”€ Credibility Score Ranges
â”œâ”€â”€ Activity Feed
â”‚   â””â”€â”€ Recent Updates List
â””â”€â”€ Quick Actions
    â”œâ”€â”€ Add Company Card
    â”œâ”€â”€ Run Analysis Card
    â””â”€â”€ Generate Report Card
```

---

## ðŸ”§ Configuration & Environment

### **Backend Environment** (`backend/.env`):
```bash
# API Settings
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=True

# Elasticsearch
ES_HOST=localhost
ES_PORT=9200

# Neo4j
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password

# MongoDB
MONGO_HOST=localhost
MONGO_PORT=27017
MONGO_DB=ecotrace

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
```

### **Frontend Environment** (`frontend/.env`):
```bash
VITE_API_URL=http://localhost:8000
```

---

## ðŸ“Š Performance Characteristics

### **API Response Times**:
| Endpoint | Avg Time | Data Size |
|----------|----------|-----------|
| /api/health | <10ms | 100 bytes |
| /api/companies/ | <50ms | 10-50 KB |
| /api/claims/ | <50ms | 20-100 KB |
| /api/search/ | <100ms | 10-200 KB |
| /api/analytics/overview | <150ms | 5-20 KB |

### **Frontend Load Times**:
| Page | Initial Load | Cached Load |
|------|--------------|-------------|
| Landing | 1-2s | <500ms |
| Dashboard | 2-3s | <1s |
| Companies | 1-2s | <500ms |
| Search | 1-2s | <500ms |

### **Database Query Performance**:
| Database | Query Type | Avg Time |
|----------|-----------|----------|
| Elasticsearch | Full-text search | <50ms |
| Neo4j | Graph traversal | <100ms |
| MongoDB | Document fetch | <20ms |
| Redis | Cache retrieval | <5ms |

---

## ðŸ” Debugging & Monitoring

### **How to Check System Health**:

1. **Check Databases**:
   ```bash
   docker ps --filter "name=ecotrace"
   # All 4 should show "Up"
   ```

2. **Check API**:
   ```bash
   curl http://localhost:8000/api/health
   # Should return {"status": "healthy"}
   ```

3. **Check Frontend**:
   ```bash
   curl http://localhost:5173
   # Should return HTML
   ```

4. **View Logs**:
   ```bash
   # API logs
   tail -f /tmp/ecotrace_api_new.log

   # Frontend logs
   tail -f /tmp/ecotrace_frontend.log
   ```

---

## ðŸŽ¯ Summary: Complete Data Flow

```
1. CRAWL
   Scrapy Spiders â†’ Web Sources â†’ Raw Data

2. PROCESS
   Pipelines â†’ Validation â†’ NLP â†’ Scoring

3. STORE
   Elasticsearch (Search)
   Neo4j (Relationships)
   MongoDB (Documents)
   Redis (Cache)

4. API
   FastAPI â†’ Query Databases â†’ Format Response

5. DISPLAY
   React â†’ Fetch Data â†’ Render UI â†’ User Sees Results

6. INTERACT
   User Actions â†’ API Calls â†’ Database Updates â†’ UI Updates
```

**Key Principle**:
```
Data flows ONE WAY: Collection â†’ Processing â†’ Storage â†’ API â†’ UI
```

---

## ðŸ“š Additional Resources

- **Full API Docs**: See `backend/api/main.py`
- **Spider Docs**: See `backend/scrapy_crawlers/ecotrace_crawler/spiders/`
- **Frontend Components**: See `frontend/src/pages/`
- **Database Schemas**: See `backend/database/`

---

**ðŸŽ‰ You now understand how the entire EcoTrace application works!**

*From web crawling to beautiful visualizations, every piece works together to verify sustainability claims.*
